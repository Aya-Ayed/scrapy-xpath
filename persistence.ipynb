{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence and Efficency\n",
    "You now might be aware of some of the problems with parsing several pages and how that can quicly get out of hand. While parsing, you might chose to persist the collected data, so that it can be analyzed and cleaned later. \n",
    "Parsing, retrieving, saving, and cleaning data are all separate actions, and you shouldn't try to work with data while collecting. In this notebook you'll practice further parsing techniques along with persistency, both for JSON and CSV formats as well as using SQL and a database.\n",
    "\n",
    "Start by loading one of the available HTML files into the `scrapy` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scrapy==2.5.1\n",
      "  Downloading Scrapy-2.5.1-py2.py3-none-any.whl (254 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.0/255.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ipykernel==6.15.2\n",
      "  Downloading ipykernel-6.15.2-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas==1.4.4\n",
      "  Downloading pandas-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib==3.5.3\n",
      "  Downloading matplotlib-3.5.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib-inline==0.1.6 in /home/vscode/.local/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (0.1.6)\n",
      "Collecting pyOpenSSL==22.0.0\n",
      "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography==37.0.4\n",
      "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting Twisted==17.9.0\n",
      "  Downloading Twisted-17.9.0.tar.bz2 (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting parsel>=1.5.0\n",
      "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting Twisted[http2]>=17.9.0\n",
      "  Downloading twisted-23.10.0-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting h2<4.0,>=3.0\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Downloading zope.interface-6.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.3/249.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lxml>=3.5.0\n",
      "  Downloading lxml-5.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: packaging in /home/vscode/.local/lib/python3.8/site-packages (from ipykernel==6.15.2->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/vscode/.local/lib/python3.8/site-packages (from ipykernel==6.15.2->-r requirements.txt (line 2)) (8.6.0)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /home/vscode/.local/lib/python3.8/site-packages (from ipykernel==6.15.2->-r requirements.txt (line 2)) (5.14.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in /home/vscode/.local/lib/python3.8/site-packages (from ipykernel==6.15.2->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/vscode/.local/lib/python3.8/site-packages (from ipykernel==6.15.2->-r requirements.txt (line 2)) (8.12.3)\n",
      "Requirement already satisfied: nest-asyncio in /home/vscode/.local/lib/python3.8/site-packages (from ipykernel==6.15.2->-r requirements.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/vscode/.local/lib/python3.8/site-packages (from ipykernel==6.15.2->-r requirements.txt (line 2)) (6.4)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/vscode/.local/lib/python3.8/site-packages (from ipykernel==6.15.2->-r requirements.txt (line 2)) (25.1.2)\n",
      "Requirement already satisfied: psutil in /home/vscode/.local/lib/python3.8/site-packages (from ipykernel==6.15.2->-r requirements.txt (line 2)) (5.9.8)\n",
      "Collecting numpy>=1.18.5\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/vscode/.local/lib/python3.8/site-packages (from pandas==1.4.4->-r requirements.txt (line 3)) (2.8.2)\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.47.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading pillow-10.2.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cffi>=1.12\n",
      "  Downloading cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (444 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.7/444.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting constantly>=15.1\n",
      "  Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Using cached incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Automat>=0.3.0\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting attrs>=19.2.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /home/vscode/.local/lib/python3.8/site-packages (from Automat>=0.3.0->Twisted==17.9.0->-r requirements.txt (line 8)) (1.16.0)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting idna>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/vscode/.local/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (3.0.43)\n",
      "Requirement already satisfied: pickleshare in /home/vscode/.local/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/vscode/.local/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/vscode/.local/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (2.17.2)\n",
      "Requirement already satisfied: backcall in /home/vscode/.local/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /home/vscode/.local/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/vscode/.local/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: decorator in /home/vscode/.local/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: typing-extensions in /home/vscode/.local/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (4.9.0)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/vscode/.local/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel==6.15.2->-r requirements.txt (line 2)) (5.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /home/vscode/.local/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel==6.15.2->-r requirements.txt (line 2)) (7.0.1)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Twisted[http2]>=17.9.0\n",
      "  Downloading twisted-23.8.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading Twisted-22.8.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading Twisted-22.4.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading Twisted-22.2.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading Twisted-22.1.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading Twisted-21.7.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading Twisted-21.2.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading Twisted-20.3.0.tar.bz2 (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading Twisted-19.10.0.tar.bz2 (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading Twisted-19.7.0.tar.bz2 (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading Twisted-19.2.1.tar.bz2 (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading Twisted-19.2.0.tar.bz2 (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading Twisted-18.9.0.tar.bz2 (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading Twisted-18.7.0.tar.bz2 (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading Twisted-18.4.0.tar.bz2 (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting priority<2.0,>=1.1.0\n",
      "  Downloading priority-1.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from zope.interface>=4.1.3->scrapy==2.5.1->-r requirements.txt (line 1)) (67.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/vscode/.local/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel==6.15.2->-r requirements.txt (line 2)) (3.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/vscode/.local/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/vscode/.local/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel==6.15.2->-r requirements.txt (line 2)) (4.1.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/vscode/.local/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/vscode/.local/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/vscode/.local/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (2.0.1)\n",
      "Requirement already satisfied: pure-eval in /home/vscode/.local/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/vscode/.local/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel==6.15.2->-r requirements.txt (line 2)) (2.4.1)\n",
      "Building wheels for collected packages: Twisted\n",
      "  Building wheel for Twisted (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Twisted: filename=Twisted-17.9.0-cp38-cp38-linux_x86_64.whl size=3022187 sha256=99491270c0dcc0161e975a68e0c086b84da9fa840cc660ca6ce42b4ae30cd462\n",
      "  Stored in directory: /home/vscode/.cache/pip/wheels/00/46/3d/0fa1ac682547517552856a3f6b5d3a32f31194be6633fc2cc1\n",
      "Successfully built Twisted\n",
      "Installing collected packages: pytz, PyDispatcher, priority, incremental, hyperframe, hpack, zope.interface, w3lib, queuelib, pyparsing, pycparser, pyasn1, protego, pillow, numpy, lxml, kiwisolver, jmespath, itemadapter, idna, h2, fonttools, cycler, cssselect, constantly, attrs, pyasn1-modules, parsel, pandas, matplotlib, hyperlink, cffi, Automat, Twisted, itemloaders, cryptography, service-identity, pyOpenSSL, ipykernel, scrapy\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.29.0\n",
      "    Uninstalling ipykernel-6.29.0:\n",
      "      Successfully uninstalled ipykernel-6.29.0\n",
      "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-17.9.0 attrs-23.2.0 cffi-1.16.0 constantly-23.10.4 cryptography-37.0.4 cssselect-1.2.0 cycler-0.12.1 fonttools-4.47.2 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 hyperlink-21.0.0 idna-3.6 incremental-22.10.0 ipykernel-6.15.2 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 kiwisolver-1.4.5 lxml-5.1.0 matplotlib-3.5.3 numpy-1.24.4 pandas-1.4.4 parsel-1.8.1 pillow-10.2.0 priority-1.3.0 protego-0.3.0 pyOpenSSL-22.0.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 pycparser-2.21 pyparsing-3.1.1 pytz-2023.3.post1 queuelib-1.6.2 scrapy-2.5.1 service-identity-24.1.0 w3lib-2.1.2 zope.interface-6.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# If you haven't already created an activated a virtual environment for this notebook, run this cell\n",
    "!python3 -m venv venv\n",
    "!source venv/bin/activate\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import os\n",
    "current_dir = os.path.abspath('')\n",
    "url = os.path.join(current_dir, \"html/1992_World_Junior_Championships_in_Athletics_–_Men's_high_jump\")\n",
    "with open(url) as _f:\n",
    "    url_data = _f.read()\n",
    "\n",
    "response = scrapy.http.TextResponse(url, body=url_data, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Steve Smith\n",
      "Silver Tim Forsyth\n",
      "Bronze Takahiro Kimino\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the interesting data is available \n",
    "table = response.xpath('//table')[1].xpath('tbody')\n",
    "for tr in table.xpath('tr'):\n",
    "    print(tr.xpath('td/b/text()').extract()[0],\n",
    "          tr.xpath('td/a/text()').extract()[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interaction with `scrapy` in a Jupyter Notebook is useful because you don't need to run the special shell and you also don't need to run the whole spider. Once you learn what you need to do here, you can adapat the spider to persist data.\n",
    "First, start by persisting data as JSON. To do this, you will need to keep the information in a Python data structure like a dictionary, and then load it as a JSON object, and finally, save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gold': 'Steve Smith', 'Silver': 'Tim Forsyth', 'Bronze': 'Takahiro Kimino'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrapped_data = {}\n",
    "for tr in table.xpath('tr'):\n",
    "    medal = tr.xpath('td/b/text()').extract()[0]\n",
    "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
    "    scrapped_data[medal] = athlete\n",
    "\n",
    "scrapped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# You can convert Python into JSON first, but there is no need if you use `json.dump()`\n",
    "# as shown next\n",
    "json_data = json.dumps(scrapped_data)\n",
    "\n",
    "# Persist it in a file:\n",
    "with open(\"1992_results.json\", \"w\") as _f:\n",
    "    # use dump() with the Python dictionary directly. \n",
    "    # the conversion is done on the fly\n",
    "    json.dump(scrapped_data, _f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can persist the scrapped data as JSON, you can also use CSV. This is specially useful if you want to to some data science operations. Although you can use an advanced library like Pandas for this, you can use the standard library CSV module from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the data first\n",
    "\n",
    "column_names = [\"Medal\", \"Athlete\"]\n",
    "rows = []\n",
    "\n",
    "for tr in table.xpath('tr'):\n",
    "    medal = tr.xpath('td/b/text()').extract()[0]\n",
    "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
    "    rows.append([medal, athlete])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now persist it to disk\n",
    "import csv\n",
    "\n",
    "with open(\"1992_results.csv\", \"w\") as _f:\n",
    "    writer = csv.writer(_f)\n",
    "\n",
    "    # write the column names\n",
    "    writer.writerow(column_names)\n",
    "\n",
    "    # now write the rows\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can persist data to a database. Unlike the JSON and CSV approach, using a database is much more memory efficient. This is the principal reason why you want to use a database instead of a file on disk. Imagine capturing 10GB of data. This could potentially mean that you need 10GB of available memory to hold onto that data before saving it to disk.\n",
    "By using a database, you can save the data as the data is gathered. \n",
    "\n",
    "For the next cells, use a SQLite database to persist the data. Create the file-based database and the table needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "connection = sqlite3.connect(\"1992_results.db\")\n",
    "db_table = 'CREATE TABLE results (id integer primary key, medal TEXT, athlete TEXT)'\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(db_table)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it is time to persist the data. Open the connection again\n",
    "connection = sqlite3.connect(\"1992_results.db\")\n",
    "cursor = connection.cursor()\n",
    "query = 'INSERT INTO results(medal, athlete) VALUES(?, ?)'\n",
    "\n",
    "for tr in table.xpath('tr'):\n",
    "    medal = tr.xpath('td/b/text()').extract()[0]\n",
    "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
    "    cursor.execute(query, (medal, athlete)) \n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Gold', 'Steve Smith'),\n",
       " (2, 'Silver', 'Tim Forsyth'),\n",
       " (3, 'Bronze', 'Takahiro Kimino')]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection = sqlite3.connect(\"1992_results.db\")\n",
    "cursor = connection.cursor()\n",
    "query = 'SELECT * FROM results'\n",
    "cursor.execute(query)\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now persisted in a file-based database that you can query. Verify that all works by creating a new connection and querying the database.\n",
    "\n",
    "Update the _wikipedia_ project and spider to use some of these techniques to persist data. Next, try parsing all the files in the _html_ directory instead of just one and persist all results. Do you think you can parse other information as well? \n",
    "\n",
    "Try parsing the height and the results from all the other athletes, not just the top three places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rank', 'Name', 'Nationality', 'Result']\n",
      " 1,  Steve Smith ,  United Kingdom  , 2.37\n",
      " 2,  Tim Forsyth ,  Australia  , 2.31\n",
      " 3,  Takahiro Kimino ,  Japan  , 2.29\n",
      " 4,  Kim Tae Young ,  South Korea  , 2.23\n",
      " 5,  Sergey Klyugin ,  Commonwealth of Independent States  , 2.20\n",
      " 6,  Kristofer Lamos ,  Germany  , 2.20\n",
      " 7,  Tomáš Janků ,  Czechoslovakia  , 2.17\n",
      " 8,  Coenraad Roux ,  South Africa  , 2.17\n",
      " 9,  Clayton Pugh ,  Australia  , 2.17\n",
      "10,  Xu Xiaodong ,  China  , 2.14\n",
      "11,  Sven Ootjers ,  Netherlands  , 2.14\n",
      "12,  Mirko Zanotti ,  Italy  , 2.14\n",
      "13,  Clifford van Reed ,  United States  , 2.14\n",
      "14,  Dejan Miloševic ,  Slovenia  , 2.10\n",
      "15,  Hugo Muñoz ,  Peru  , 2.10\n",
      "16,  Stanley Osuide ,  United Kingdom  , 2.05\n",
      "17,  Kostas Liapis ,  Greece  , 2.05\n",
      "18,  Kim Tae-hoi ,  South Korea  , 2.05\n",
      "19,  Antoine Burke ,  Ireland  , 2.00\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the interesting data is available \n",
    "table = response.xpath('//table')[2].xpath('tbody')\n",
    "print(table.xpath('tr/th/text()').extract()[:4])\n",
    "\n",
    "rank=1\n",
    "\n",
    "for tr in table.xpath('tr'):\n",
    "      try: \n",
    "            print(f\"{rank:2d}, \",\n",
    "            tr.xpath('td//a/text()').extract()[0],\", \",\n",
    "            tr.xpath('td//a/text()').extract()[1],\" ,\",\n",
    "            tr.xpath('td/b/text()').extract()[0]\n",
    "            )\n",
    "            rank+=1\n",
    "      except IndexError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = response.xpath('//table')[2].xpath('tbody')\n",
    "column_names= table.xpath('tr/th/text()').extract()[:4]\n",
    "rows = []\n",
    "\n",
    "rank=1\n",
    "\n",
    "for tr in table.xpath('tr'):\n",
    "      try: \n",
    "    \n",
    "            athlete= tr.xpath('td//a/text()').extract()[0]\n",
    "            nationality= tr.xpath('td//a/text()').extract()[1]\n",
    "            result= tr.xpath('td/b/text()').extract()[0]\n",
    "            rows.append([rank, athlete, nationality, result])\n",
    "            rank+=1\n",
    "      except IndexError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"1992_results_all.csv\",\"w\") as _f:\n",
    "    writer = csv.writer(_f)\n",
    "    writer.writerow(column_names)\n",
    "    writer.writerows(rows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0303190f1c8c691fa9994d3c799a212d90e80d675371cad4184da4200e89748e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('scraping': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
